<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Your description goes here" />
	<meta name="keywords" content="your,keywords,goes,here" />
	<meta name="author" content="Your Name" />
	<link rel="stylesheet" type="text/css" href="projects.css" title="projects" media="screen,projection" />
	<title>Explore my projects</title>
</head>

<body>
<div id="container">
	<div id="sitename">
		<h1>Shankar Jagadeesan</h1>
		<h2>Teaching machines to see and respond</h2>
	</div>
	<div id="mainmenu">
		<ul>
			<li><a href="index.html">Home Page</a></li>
			<li><a href="dl.html">Deep Learning</a></li>
			<li><a class="current" href="research.html">Research Work</a></li>
			<li><a href="mv.html">Machine Vision</a></li>
			<li><a href="pp.html">Patents & Pubs</a></li>
			<li><a href="rovio/index.html" target="_blank">Robotics (CVRL Intern)</a></li>
			<li><a href="grad.html">Grad school (old stuff)</a></li>
		</ul>
	</div>
 
	<div id="wrap">
		<div id="rightside">
			<p>
				&nbsp;<img src="img/profile.jpg" height="104" width="104" alt="Profile Picture" 
						style="margin-left: 0px" class="thumbnail"/></p>
				<h1>Links:</h1>
				<ul class="linklist">
					<li><a href="https://shanksvision.github.io/">Home</a></li>
					<li><a href="http://linkedin.com/in/shankaranandj">LinkedIn</a></li>
					<li><a href="https://github.com/ShanksVision">GitHub</a></li>
					<li><a href="http://vision.ece.uic.edu/">Lab</a></li>
					<li><a href="https://www.cognex.com/products/deep-learning">Work</a></li>				
					<li><a href="https://open.spotify.com/user/xaqflm5lmkitr6npwq22bozsd?si=UkQA8qOcSXihMcXTqceDCQ">Music</a></li>
					<li><a href="https://cricclubs.com/NCCA/viewPlayer.do?playerId=407206&clubId=1191#parentHorizontalTab2">Sport</a></li>
				</ul>
		</div>

		<div id="contentalt">
			<h1>An Efficient Methodology for 3D Tracking and Poinitng Localization of Humans for Robotic Guidance</h1>
			<p>Human gestures are one of the primary ways of communication in Human-Machine Interactions. 
			In my study of “Effective communication with robotic assistants for the elderly: Integrating Speech, Vision and Haptics”, 
			I concentrated on interpreting the human pointing gestures. My thesis study is about the post-detection stage of a human pointing gesture which is 
			essentially determining the human pointing targets. </p><br />

			<p>In this work, I investigated the calibration of a multi-camera environment for a perfect 3D reconstruction of an ADL (Activities of Daily Living) 
				room. This calibration process was inspired from the work of Tomas Svoboda’s Self-Calibration of virtual environment. 
				The calibration process involves synchronized data capture of the calibration object, projective reconstruction by factorization, 
				projective depth estimation, Euclidean stratification, estimation of non-linear distortion and finally alignment with the world coordinate system. 
				The aligned coordinate system will provide the location of each object in the room with precise 3D coordinates in metric units.</p>
				<br />
			<p>Our work (multi-lab coalition) is a part of the NSF project that deals with Integrating Speech, Vision and Haptics. 
				Thus we propose a multi-modal method to detect human pointing gestures and implement a vision based method that interprets the pointing and 
				finds the pointed target location accurately. Two approaches of determining the pointing direction are used in this work; one is based on the 
				forearm and the other one is based on index finger. 
				In our experiments with the elderly in ADL Room we found that only these two pointing gestures are predominantly used. 
				In my research I develop methods to find the pointed objects by using a 3D database of object locations in the room. 
				I also compare the results of finger and forearm based pointing. 
				The proposed method would also list the closest objects if the computed pointing location is vacant. </p>	<br />

				<p>I also proposed and implemented a method for detecting human head in upright pose and
				tracking the location of the person in 3D inside a calibrated environment. 
				I obtain the trajectories of the person as function of time. 
				The trajectory information along with the pointing information can be used in conjunction with speech to disambiguate communication 
				with the robotic assistant. </p>

			<p><h3><a href="https://github.com/ShanksVision/3DPointingLocalization">Link to thesis and presentation in github repo</a></h3></p>

				<br />

			<p> <h3>Videos of the implementation shot @ Machine Vison Lab, UIC</h3></p>

			<br />

			<div><iframe width="560" height="315" src="https://www.youtube.com/embed/LwhxPbJ_KkM" title="YouTube video player" 
				frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>

			<br /><br />

			<div>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/Livzi5d1byU" title="YouTube video player" 
					frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>



			<div class="clearingdiv">&nbsp;</div>
	</div>
</div>

<div id="footer">
	<p> Reach out/connect <a href="http://linkedin.com/in/shankaranandj">here</a> | <a href="https://shanksvision.github.io/">Home Page</a></p>
</div>

</body>
</html>